import org.apache.spark.SparkContext
import org.apache.spark.sql.catalyst.dsl.expressions.doubleToLiteral
import org.apache.spark.storage.StorageLevel
import org.apache.log4j.Logger
import org.apache.log4j.Level

object scenior1 {

  def main(args:Array[String]):Unit={

    Logger.getLogger("org").setLevel(Level.OFF)
//    val sc = new SparkContext("local[8]", "karthik")
//    val rdd = sc.parallelize(Array(11, 22, 33, 44, 55, 6, 9, 12, 18, 20, 25, 29))
//    val filteredRdd = rdd.filter(x => x % 11 == 0) // filter out the records which is given in array
//
//    val count = filteredRdd.count()
//    println("Count of divisible by 11 numbers: " + count)
//    filteredRdd.collect.foreach(println) // 11,22,33,44,55

//    val sc = new SparkContext("local[8]", "karthik")
//    val rdd = sc.parallelize(Array(11, 22, 33, 44, 55, 6, 9, 12, 18, 20, 25, 29))
//    val filteredRdd = rdd.filter(x => x % 3 == 0) // filter out the records which is given in array
//
//    val count = filteredRdd.count()
//    println("Count of divisible by 3 numbers: " + count)  //5
//    filteredRdd.collect.foreach(println) // 33,6,9,12,18


//    val sc = new SparkContext("local[8]", "karthik")
//    val rdd = sc.parallelize(Array(11, 22, 33, 44, 55, 6, 9, 12, 18, 20, 25, 29))
//    val filteredRdd = rdd.filter(x => x % 3 ||x % 11 == 0 )

//    val count = filteredRdd.count()
//
//    println("Count of divisible by 3 &11 numbers: " + count)
//    filteredRdd.collect.foreach(println)


      //to find the total revenue generated by each customer

//    val rdd = sc.parallelize(Array(1, 2, 3, 4, 5))
//        val sum = rdd.reduce(_ + _)
//        val count = rdd.count()
//            println("sum of the value: " + sum)





  }
}
